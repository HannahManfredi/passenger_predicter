{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadMode\n",
    "import torch \n",
    "import plotly.express as px\n",
    "from IPython.display import Audio as DisplayAudio, display\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
    "from transformers import TrainingArguments, Trainer, Wav2Vec2FeatureExtractor, Wav2Vec2Model, AutoModelForAudioClassification\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor, pipeline\n",
    "import datasets\n",
    "from datasets import Audio, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 1819/1819 [00:00<00:00, 2958.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  1819\n",
      "type:  <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'rating1': Value(dtype='string', id=None),\n",
       " 'rating2': Value(dtype='string', id=None),\n",
       " 'rating3': Value(dtype='string', id=None),\n",
       " 'gender': Value(dtype='string', id=None),\n",
       " 'valence': Value(dtype='float32', id=None),\n",
       " 'activation': Value(dtype='float32', id=None),\n",
       " 'dominance': Value(dtype='float32', id=None),\n",
       " 'label': ClassLabel(names=['neu', 'fru', 'sad', 'sur', 'ang', 'hap', 'exc', 'fea', 'dis', 'xxx', 'oth'], id=None)}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"../iemocap\", split=\"train\", trust_remote_code=True, download_mode=DownloadMode.FORCE_REDOWNLOAD).with_format(\"torch\", device=device)\n",
    "num_rows = dataset.num_rows\n",
    "print('number of rows: ', num_rows)\n",
    "print('type: ', type(dataset))\n",
    "dataset.features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/Users/hannahmanfredi/.cache/huggingface/datasets/downloads/extracted/8ae19e95b6f4b1c54cfe7c431eaa1ac802ae3e5691b37f3a703719cf096e8838/iemocap/Session1/sentences/wav/Ses01M_impro01/Ses01M_impro01_F010.wav',\n",
       " 'array': tensor([ 1.5259e-03,  6.1035e-05, -1.3428e-03,  ..., -9.7656e-04,\n",
       "         -2.3804e-03, -3.7842e-03], device='mps:0'),\n",
       " 'sampling_rate': tensor(16000, device='mps:0')}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_array_wrapper = dataset[10][\"audio\"]\n",
    "audio_array_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(DisplayAudio(audio_array_wrapper['array'], rate=audio_array_wrapper['sampling_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the HuggingFace dataset to a pandas DataFrame\n",
    "# df = dataset.to_pandas()\n",
    "# df.head()\n",
    "# df['label'] = df['label'].map(lambda label: dataset.features[\"label\"].names[label])\n",
    "\n",
    "# fig = px.scatter_3d(df, x='valence', y='activation', z='dominance', color='label')\n",
    "# fig.update_layout(\n",
    "#     autosize=False,\n",
    "#     width=1000,\n",
    "#     height=1000,\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Histogram to visualize the distribution of the labels\n",
    "# fig = px.histogram(df, x='label', title='Initial Label Distribution')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset by removing outliers\n",
    "\n",
    "# Count the number of occurrences of each label\n",
    "# label_counts = df['label'].value_counts()\n",
    "\n",
    "# # Define a threshold for the minimum number of samples per label\n",
    "# threshold = 100\n",
    "\n",
    "# # Filter out labels with fewer than the threshold number of samples\n",
    "# df = df.groupby('label').filter(lambda x: len(x) > threshold)\n",
    "\n",
    "# fig = px.histogram(df, x='label', title='Balanced Label Distribution')\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(6, device='mps:0'), 'Excited', 'Surprise', 'Excited']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "sampling_rate = 16000\n",
    "sample = dataset[192]\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"hughlan1214/Speech_Emotion_Recognition_wav2vec2-large-xlsr-53_240304_SER_fine-tuned2.0\")\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"hughlan1214/Speech_Emotion_Recognition_wav2vec2-large-xlsr-53_240304_SER_fine-tuned2.0\")\n",
    "dataset = dataset.remove_columns(['text', 'rating1', 'rating2', 'rating3', 'gender', 'valence', 'activation', 'dominance'])\n",
    "inputs = feature_extractor(sample[\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "# def prepare_dataset(examples):\n",
    "#    # for each ex. in my batch, go into my audio obj and get my raw audio data\n",
    "#    audio_arrays = [x[\"array\"].cpu().numpy() for x in examples[\"audio\"]]\n",
    "#    inputs = processor(\n",
    "#         audio_arrays,\n",
    "#         sampling_rate=sampling_rate,\n",
    "#         truncation=True,\n",
    "#         padding=\"longest\",\n",
    "#         max_length=3 * sampling_rate,\n",
    "#     )\n",
    "#    return inputs\n",
    "\n",
    "# #apply our processor to our dataset\n",
    "# dataset = dataset.map(prepare_dataset, remove_columns='audio', batched=True, batch_size=batch_size)\n",
    "# inputs = dataset\n",
    "inputs\n",
    "[sample[\"label\"], sample[\"rating1\"], sample[\"rating2\"], sample[\"rating3\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_ids = torch.argmax(logits).item()\n",
    "predicted_label = model.config.id2label[predicted_class_ids]\n",
    "predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_classifier-zethqTI6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
